import os
import csv
import time
from datetime import datetime
import streamlit as st

try:
    from code_indexer import embed_project
except ImportError:
    st.error(
        "'code_indexer.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
    )
    st.stop()

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# ì„¤ì •
BASE_DB_PATH = "./chroma_db"
EMBEDDING_MODEL_NAME = "BAAI/bge-small-en-v1.5"
OLLAMA_MODEL_NAME = "qwen2.5-coder:7b"
OLLAMA_BASE_URL = "http://localhost:11434"
FEEDBACK_FILE = "rag_feedback.csv"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI Co-Developer", layout="wide")
st.markdown(
    """
### AI Co-Developer: ë„ë©”ì¸ì„ ë„˜ë‚˜ë“œëŠ” ì½”ë”© íŒŒíŠ¸ë„ˆ
ë°±ì—”ë“œ, í”„ë¡ íŠ¸ì—”ë“œ, DB ë“± **ì „ì²´ í”„ë¡œì íŠ¸ ë§¥ë½ì„ ì´í•´í•˜ê³  í˜‘ì—…**í•˜ëŠ” ë¡œì»¬ AI ë„êµ¬ì…ë‹ˆë‹¤.
"""
)


# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def get_existing_projects():
    """chroma_db í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ í•™ìŠµëœ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not os.path.exists(BASE_DB_PATH):
        return []
    # í´ë”ì´ë©´ì„œ ìˆ¨ê¹€ íŒŒì¼ì´ ì•„ë‹Œ ê²ƒë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ì—…
    projects = [
        d
        for d in os.listdir(BASE_DB_PATH)
        if os.path.isdir(os.path.join(BASE_DB_PATH, d)) and not d.startswith(".")
    ]
    return sorted(projects)


# í•¨ìˆ˜: íŒŒì¼ íŠ¸ë¦¬ ìƒì„± (Context Map)
def generate_file_tree(startpath):
    """í”„ë¡œì íŠ¸ì˜ ì „ì²´ ì§€ë„ë¥¼ ê·¸ë ¤ì£¼ì–´, ê°œë°œìê°€ ì–´ë””ë¥¼ ìˆ˜ì •í•´ì•¼ í• ì§€ ìœ„ì¹˜ë¥¼ íŒŒì•…í•˜ê²Œ ë•ìŠµë‹ˆë‹¤."""
    if not startpath or not os.path.exists(startpath):
        return "(ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.)"
    tree_str = ""
    for root, dirs, files in os.walk(startpath):
        dirs[:] = [d for d in dirs if not d.startswith(".")]  # ìˆ¨ê¹€ í´ë” ì œì™¸
        level = root.replace(startpath, "").count(os.sep)
        indent = " " * 4 * (level)
        base = os.path.basename(root)
        if base:
            subindent = " " * 4 * (level + 1)
            tree_str += f"{indent} {base}/\n"
            for f in files:
                if not f.startswith("."):
                    tree_str += f"{subindent} {f}\n"
    return tree_str if tree_str else "(êµ¬ì¡°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"


# ì‚¬ì´ë“œë°”: ì‚¬ìš©ì ë° ì„œë²„ ì„¤ì •
with st.sidebar:
    st.header("í”„ë¡œì íŠ¸ ì„ íƒ")

    # 1. ê¸°ì¡´ í”„ë¡œì íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    existing_projects = get_existing_projects()

    # 2. í”„ë¡œì íŠ¸ ì„ íƒ ë°©ì‹
    tab1, tab2 = st.tabs(["ë¶ˆëŸ¬ì˜¤ê¸°", "ìƒˆë¡œë§Œë“¤ê¸°"])

    with tab1:
        if existing_projects:
            select_project = st.selectbox(
                "í•™ìŠµëœ í”„ë¡œì íŠ¸ ì„ íƒ", existing_projects, index=0
            )
            project_name = select_project
            st.success(f"'{project_name}' ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ")
        else:
            st.info("ì•„ì§ í•™ìŠµëœ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. 'ìƒˆë¡œ ë§Œë“¤ê¸°' íƒ­ì„ ì´ìš©í•˜ì„¸ìš”.")
            project_name = None
    with tab2:
        new_project_name = st.text_input(
            "ìƒˆ í”„ë¡œì íŠ¸ ì´ë¦„(DBëª…)", placeholder="ì˜ˆ: my-new-project"
        )
        new_root_path = st.text_input(
            "ì‹¤ì œ íŒŒì¼ ê²½ë¡œ (Root Path)", placeholder="C:/Work/MyProject"
        )

        if st.button("DB í•™ìŠµ ì‹œì‘", type="primary"):
            if not new_project_name or not new_root_path:
                st.error("ì´ë¦„ê³¼ ê²½ë¡œë¥¼ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
            else:
                with st.spinner(f"'{new_project_name}' í•™ìŠµ ì¤‘."):
                    success, msg = embed_project(new_root_path, new_project_name)
                    if success:
                        st.success(msg)
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(msg)
        # íƒ­2ê°€ í™œì„±í™” ë˜ì—ˆê³  ì…ë ¥ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ í”„ë¡œì íŠ¸ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        if new_project_name and not project_name:
            project_name = new_project_name

    # íŒŒì¼ íŠ¸ë¦¬ ê²½ë¡œ (ë¶ˆëŸ¬ì˜¤ê¸° ëª¨ë“œì¼ ë•Œë„ íŠ¸ë¦¬ë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´ ê²½ë¡œ ì…ë ¥ í•„ìš”)
    # DBì—ëŠ” íŒŒì¼ ë‚´ìš©ë§Œ ìˆê³  íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ì‹¤ì œ ê²½ë¡œëŠ” ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì…ë ¥ë°›ìŒ
    st.divider()
    project_root_path = st.text_input(
        "íŒŒì¼ íŠ¸ë¦¬ ê²½ë¡œ",
        help="í˜„ì¬ í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ë©´ íŒŒì¼ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.",
    )

    user_id = st.text_input("ê°œë°œì ID", value="Dev User")

    if project_root_path and os.path.isdir(project_root_path):
        with st.expander("íŒŒì¼ êµ¬ì¡° ë³´ê¸°"):
            st.code(generate_file_tree(project_root_path), language="text")


# RAG íŒŒì´í”„ë¼ì¸ ë¡œë“œ
@st.cache_resource
def load_rag_pipeline(prj_name):
    if not prj_name:
        return None, "í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”."

    db_path = os.path.join(BASE_DB_PATH, prj_name)
    if not os.path.exists(db_path):
        return None, f"'{prj_name}' DBê°€ ì—†ìŠµë‹ˆë‹¤."

    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    try:
        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 7})
        llm = Ollama(model=OLLAMA_MODEL_NAME, base_url=OLLAMA_BASE_URL)
        return retriever, llm
    except Exception as e:
        return None, str(e)


# í”¼ë“œë°± ë¡œê¹…
def log_feedback(project, user, question, answer, rating, docs):
    file_exists = os.path.isfile(FEEDBACK_FILE)
    with open(FEEDBACK_FILE, mode="a", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        if not file_exists:
            writer.writerow(
                [
                    "Time",
                    "Project",
                    "User",
                    "Question",
                    "Answer",
                    "Rating",
                    "Context_Files",
                ]
            )

        sources = [d.metadata.get("source") for d in docs]
        writer.writerow(
            [datetime.now(), project, user, question, answer, rating, str(sources)]
        )


# í”„ë¡¬í”„íŠ¸ (í˜‘ì—… ë° ì„¤ëª… ì¤‘ì‹¬)
PROMPT_TEMPLATE = """
ë‹¹ì‹ ì€ ì´ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê¸°ìˆ  ìŠ¤íƒ(Full-stack)ì„ ì´í•´í•˜ê³  ìˆëŠ” **ìˆ˜ì„ í…Œí¬ ë¦¬ë“œ(Tech Lead)**ì…ë‹ˆë‹¤.
ì‚¬ìš©ìëŠ” íŠ¹ì • ë¶„ì•¼ì— ìµìˆ™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: ë°±ì—”ë“œ ê°œë°œìê°€ DBë¥¼ ë¬»ê±°ë‚˜, ì•± ê°œë°œìê°€ ì„œë²„ë¥¼ ë¬¼ì„ ìˆ˜ ìˆìŒ)
ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ, ì „ì²´ êµ¬ì¡° ê´€ì ì—ì„œ ë‹µë³€í•˜ì„¸ìš”.

[í”„ë¡œì íŠ¸ êµ¬ì¡°ë„]:
{file_tree}

[ì°¸ê³  ì½”ë“œ ë§¥ë½]:
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]: {question}

[ë‹µë³€ ê°€ì´ë“œ]:
1. **ì—°ê²°ì„± ê°•ì¡°:** ì§ˆë¬¸í•œ ì½”ë“œê°€ í”„ë¡œì íŠ¸ì˜ ë‹¤ë¥¸ ë¶€ë¶„(DB, API, UI ë“±)ê³¼ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ ì„¤ëª…í•˜ì„¸ìš”.
2. **ìœ„ì¹˜ ì•ˆë‚´:** ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•´ì•¼ í•œë‹¤ë©´, [í”„ë¡œì íŠ¸ êµ¬ì¡°ë„]ë¥¼ ë³´ê³  ì •í™•í•œ íŒŒì¼ ìœ„ì¹˜ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
3. **ìƒì„¸ ì„¤ëª…:** ì‚¬ìš©ìê°€ í•´ë‹¹ ì–¸ì–´ë¥¼ ì˜ ëª¨ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³ , ë¡œì§ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
4. **í•œêµ­ì–´ í•„ìˆ˜:** ëª¨ë“  ì„¤ëª…ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.

[ë‹µë³€]:
"""


def format_docs(docs):
    return "\n\n".join(
        [
            f"[íŒŒì¼: {d.metadata.get('source')}]\n```\n{d.page_content}\n```"
            for d in docs
        ]
    )


# ë©”ì¸ ì‹¤í–‰ ë¡œì§
retriever, llm = None, None
is_ready = False
system_msg = ""
current_tree = ""

# íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹œë„
if project_name:
    result = load_rag_pipeline(project_name)
    if isinstance(result, tuple):
        retriever, llm = result
        is_ready = True
        # íŠ¸ë¦¬ ìƒì„±
        if project_root_path:
            current_tree = generate_file_tree(project_root_path)
    else:
        # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë©”ì‹œì§€ë§Œ ì €ì¥í•˜ê³  ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
        system_msg = result
else:
    system_msg = "í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œ í•™ìŠµí•´ì£¼ì„¸ìš”."

# 3. ì±„íŒ… UI í‘œì‹œ
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# í•™ìŠµì´ ì•ˆë˜ì–´ ìˆì„ ê²½ìš° ê²½ê³  ë©”ì„¸ì§€ë¥¼ ì±„íŒ…ì°½ ìƒë‹¨ì— í† ìŠ¤íŠ¸ë‚˜ ê²½ê³ ë¡œ ì‚´ì§ ë³´ì—¬ì¤Œ
if not is_ready and system_msg:
    st.info(f"{system_msg}")

# 4. ì…ë ¥ì°½ ë° ë‹µë³€ ë¡œì§
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    # ì‚¬ìš©ì ë©”ì„¸ì§€ ì¦‰ì‹œ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        # ì¤€ë¹„ê°€ ì•ˆëœ ê²½ìš° ì•ˆë‚´ ë©”ì„¸ì§€ ì¶œë ¥
        if not is_ready:
            st.error("AIê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            custom_prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)
            chain = RunnableParallel(
                {
                    "context": retriever
                    | (lambda docs: "\n".join([d.page_content for d in docs])),
                    "question": RunnablePassthrough(),
                    "file_tree": lambda x: current_tree,
                }
            ).assign(answer=custom_prompt | llm | StrOutputParser())

            try:
                with st.spinner("ë¶„ì„ ì¤‘..."):
                    result = chain.invoke(prompt)
                    # ë‹µë³€ í‘œì‹œ
                    st.markdown(result["answer"])

                    # ê·¼ê±° í‘œì‹œ

                    with st.expander("ğŸ” AIê°€ ì°¸ê³ í•œ íŒŒì¼ ë° ê·¼ê±°"):
                        raw_docs = retriever.invoke(prompt)
                        for doc in raw_docs:
                            st.caption(f"{doc.metadata.get('source')}")
                            st.code(doc.page_content)

                    # ì±„íŒ… ê¸°ë¡ ì €ì¥
                    st.session_state.messages.append(
                        {"role": "assistant", "content": result["answer"]}
                    )

                    # í”¼ë“œë°±ì„ ìœ„í•œ ìƒíƒœ ì €ì¥ (í‚¤ ì´ë¦„ í†µì¼)
                    st.session_state.last_interaction = {
                        "project": project_name,
                        "question": prompt,
                        "answer": result["answer"],
                        "docs": raw_docs,
                    }
                    st.rerun()

            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")


# í”¼ë“œë°± UI
if (
    is_ready
    and "last_interaction" in st.session_state
    and st.session_state.last_interaction
):
    st.divider()
    st.caption("ğŸ“¢ ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ìê°€ ê°œì„  ë°ì´í„°)")
    cols = st.columns([1, 1, 6])
    last = st.session_state.last_interaction

    if cols[0].button("ğŸ‘ ë„ì›€ë¨"):
        log_feedback(
            last["project"],
            user_id,
            last["question"],
            last["answer"],
            "Good",
            last["docs"],
        )
        st.toast("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        del st.session_state.last_interaction
        st.rerun()

    if cols[1].button("ğŸ‘ ë¶€ì¡±í•¨"):
        log_feedback(
            last["project"],
            user_id,
            last["question"],
            last["answer"],
            "Bad",
            last["docs"],
        )
        st.toast("í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        del st.session_state.last_interaction
        st.rerun()
